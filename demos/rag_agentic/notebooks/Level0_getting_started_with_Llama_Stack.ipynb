{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5ac225-ad97-4cbd-8e90-f0ad64cbf00b",
   "metadata": {},
   "source": [
    "# Level 0: Getting Started with Llama Stack\n",
    "\n",
    "In this tutorial, we will outline the necessary steps to set up your environment and prepare everything you will need in order to execute our sample notebooks as well as create your own Llama Stack client applications. In particular, we will cover installing and importing the necessary libraries, setting up the essential configuration parameters, and initializing the connection to the Llama Stack server. More advanced sections of this notebook address the setup for RAG and MCP applications.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have a running instance of the Llama Stack server (local or remote). You will need at least one preconfigured vector DB to run the RAG notebooks. For detailed llama-stack server setup instructions and for more information, please refer to our [Remote Setup Guide](../../../kubernetes/README.md) and [Local Setup Guide](../../../local_setup_guide.md), as well as to the official [Llama Stack tutorials](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf06a5b-7259-4e4b-94ef-fbb577c30f9e",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "This code requires `llama-stack` and the `llama-stack-client`, both at version `0.2.2`. Lets begin by installing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0ddc8b-d6cf-4cb3-8678-7b52ee70131e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-stack-client==0.2.2 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (0.2.2)\n",
      "Collecting llama-stack==0.2.2\n",
      "  Using cached llama_stack-0.2.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack-client==0.2.2) (4.6.2)\n",
      "Requirement already satisfied: click in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack-client==0.2.2) (8.1.8)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack-client==0.2.2) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack-client==0.2.2) (0.27.0)\n",
      "Requirement already satisfied: pandas in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack-client==0.2.2) (2.2.3)\n",
      "Requirement already satisfied: prompt-toolkit in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack-client==0.2.2) (3.0.43)\n",
      "Requirement already satisfied: pyaml in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack-client==0.2.2) (25.1.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack-client==0.2.2) (2.10.3)\n",
      "Requirement already satisfied: rich in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack-client==0.2.2) (13.9.4)\n",
      "Requirement already satisfied: sniffio in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack-client==0.2.2) (1.3.0)\n",
      "Requirement already satisfied: termcolor in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack-client==0.2.2) (3.0.1)\n",
      "Requirement already satisfied: tqdm in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack-client==0.2.2) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack-client==0.2.2) (4.12.2)\n",
      "Requirement already satisfied: blobfile in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack==0.2.2) (3.0.0)\n",
      "Requirement already satisfied: fire in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack==0.2.2) (0.7.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack==0.2.2) (0.30.2)\n",
      "Requirement already satisfied: jinja2>=3.1.6 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack==0.2.2) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack==0.2.2) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.66 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack==0.2.2) (1.74.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack==0.2.2) (1.1.0)\n",
      "Requirement already satisfied: requests in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack==0.2.2) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack==0.2.2) (75.8.0)\n",
      "Requirement already satisfied: tiktoken in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack==0.2.2) (0.9.0)\n",
      "Requirement already satisfied: pillow in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from llama-stack==0.2.2) (11.0.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->llama-stack-client==0.2.2) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->llama-stack-client==0.2.2) (1.2.0)\n",
      "Requirement already satisfied: certifi in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->llama-stack-client==0.2.2) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->llama-stack-client==0.2.2) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->llama-stack-client==0.2.2) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from jinja2>=3.1.6->llama-stack==0.2.2) (3.0.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from openai>=1.66->llama-stack==0.2.2) (0.6.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->llama-stack-client==0.2.2) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->llama-stack-client==0.2.2) (2.27.1)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from blobfile->llama-stack==0.2.2) (3.21.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from blobfile->llama-stack==0.2.2) (2.3.0)\n",
      "Requirement already satisfied: lxml>=4.9 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from blobfile->llama-stack==0.2.2) (5.2.1)\n",
      "Requirement already satisfied: filelock>=3.0 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from blobfile->llama-stack==0.2.2) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from huggingface-hub->llama-stack==0.2.2) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from huggingface-hub->llama-stack==0.2.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from huggingface-hub->llama-stack==0.2.2) (6.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from jsonschema->llama-stack==0.2.2) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from jsonschema->llama-stack==0.2.2) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from jsonschema->llama-stack==0.2.2) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from jsonschema->llama-stack==0.2.2) (0.22.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from pandas->llama-stack-client==0.2.2) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from pandas->llama-stack-client==0.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from pandas->llama-stack-client==0.2.2) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from pandas->llama-stack-client==0.2.2) (2023.3)\n",
      "Requirement already satisfied: wcwidth in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from prompt-toolkit->llama-stack-client==0.2.2) (0.2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from requests->llama-stack==0.2.2) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from rich->llama-stack-client==0.2.2) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from rich->llama-stack-client==0.2.2) (2.19.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from tiktoken->llama-stack==0.2.2) (2024.11.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->llama-stack-client==0.2.2) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ikolchin/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-stack-client==0.2.2) (1.16.0)\n",
      "Using cached llama_stack-0.2.2-py3-none-any.whl (3.7 MB)\n",
      "Installing collected packages: llama-stack\n",
      "  Attempting uninstall: llama-stack\n",
      "    Found existing installation: llama_stack 0.2.1\n",
      "    Uninstalling llama_stack-0.2.1:\n",
      "      Successfully uninstalled llama_stack-0.2.1\n",
      "Successfully installed llama-stack-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-stack-client==0.2.2 llama-stack==0.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589b1ae-9fd5-4715-a630-cc3cc52035e9",
   "metadata": {},
   "source": [
    "## Setting the Environment Variables\n",
    "\n",
    "Use the [`.env.example`](../../../.env.example) to create a new file called `.env` and ensure you add all the relevant environment variables below.\n",
    "\n",
    "\n",
    "### Environment variables required for all demos\n",
    "- `REMOTE_BASE_URL`: the URL of the remote Llama Stack server.\n",
    "- `INFERENCE_MODEL_ID`: the ID of the model to use for inference. The model must be available on your server.\n",
    "- `LOCAL_SERVER_PORT` (optional): the port of the locally running Llama Stack server. Defaults to 8321.\n",
    "- `REMOTE` (optional): defines whether a locally running or a remote instance of the Llama Stack server should be used. Only the values of 'True' and 'False' are valid. This is useful for switching between a local development environment and a remote deployment (e.g., a Kubernetes cluster). Defaults to True.\n",
    "- `TEMPERATURE` (optional): the temperature to use during inference. Defaults to 0.0.\n",
    "- `TOP_P` (optional): the top_p parameter to use during inference. Defaults to 0.95.\n",
    "- `MAX_TOKENS` (optional): the maximum number of tokens that can be generated in the completion. Defaults to 4096.\n",
    "- `STREAM` (optional): set this to True to stream the output of the model/agent and False otherwise. Defaults to True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573899bf-34cb-4f31-9fc1-48aca439fc60",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b09511e-f6dc-4a54-b037-d64adbeaa8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "import sys\n",
    "sys.path.append('..')  \n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb65ed-f368-4bb5-957b-6698fe85b829",
   "metadata": {},
   "source": [
    "## Setting Up the Server Connection\n",
    "\n",
    "Establish the connection to the Llama Stack server by initializing the wrapper client object according to the predefined settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d60fb3f3-4d04-4916-84fc-f798b059ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server @ http://localhost:8321\n"
     ]
    }
   ],
   "source": [
    "remote = os.getenv(\"REMOTE\", \"True\")\n",
    "\n",
    "if remote == \"False\":\n",
    "    local_port = os.getenv(\"LOCAL_SERVER_PORT\", 8321)\n",
    "    base_url = f\"http://localhost:{local_port}\"\n",
    "else: # any value non equal to 'False' will be considered as 'True'\n",
    "    base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "    \n",
    "print(f\"Connected to Llama Stack server @ {base_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f5ae8-8083-4896-a973-f310df129ec6",
   "metadata": {},
   "source": [
    "## Initializing the Inference Parameters\n",
    "\n",
    "Fetch the inference-related parameters from the corresponding environment variables and convert them to the format Llama Stack expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0092359-a5db-4d9a-a735-bb931ba05f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Parameters:\n",
      "\tModel: ibm-granite/granite-3.2-8b-instruct\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 4096}\n",
      "\tstream: True\n"
     ]
    }
   ],
   "source": [
    "# model_id will later be used to pass the name of the desired inference model to Llama Stack Agents/Inference APIs\n",
    "model_id = os.getenv(\"INFERENCE_MODEL_ID\")\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
