{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
   "metadata": {},
   "source": [
    "# Level 1: Foundational RAG\n",
    "\n",
    "This tutorial presents an example of executing queries with foundational (i.e., non-agentic) RAG in Llama Stack. It shows how the APIs provided by Llama Stack can be used to directly control and invoke all RAG stages, including indexing, retrieval and inference. \n",
    "For an agentic RAG tutorial, please refer to [Level3_agentic_RAG.ipynb](demos/rag_agentic/notebooks/Level3_agentic_RAG.ipynb).\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers the following steps:\n",
    "1. Indexing a collection of documents in a vector DB for later retrieval.\n",
    "2. Executing the built-in RAG tool to retrieve the document chunks relevant to a given query.\n",
    "3. Using the retrieved context to answer user queries during the inference step.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have a running instance of the Llama Stack server (local or remote) with at least one preconfigured vector DB. For more information, please refer to the corresponding [Llama Stack tutorials](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html).\n",
    "\n",
    "## Setting the Environment Variables\n",
    "\n",
    "Use the [`.env.example`](../../../.env.example) to create a new file called `.env` and ensure you add all the relevant environment variables below.\n",
    "\n",
    "In addition to the environment variables listed in the [\"Getting Started\" notebook](demos/rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb), the following should be provided for this demo to run:\n",
    " - `VDB_PROVIDER`: the vector DB provider to be used. Must be supported by Llama Stack. For this demo, we use Milvus Lite which is our preferred solution.\n",
    " - `VDB_EMBEDDING`: the embedding model to be used for ingestion and retrieval. For this demo, we use all-MiniLM-L6-v2.\n",
    " - `VDB_EMBEDDING_DIMENSION` (optional): the dimension of the embedding. Defaults to 384.\n",
    " - `VECTOR_DB_CHUNK_SIZE` (optional): the chunk size for the vector DB. Defaults to 512."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "We will start with a few imports needed for this demo only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15080a6-48be-4475-8813-c584701d69bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from llama_stack_client import RAGDocument\n",
    "from llama_stack_client.types.shared.content_delta import TextDelta, ToolCallDelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e8c70-6f28-440b-b71a-85d4040ffac4",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](demos/rag_agentic/notebooks/Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server @ http://localhost:8321\n",
      "Inference Parameters:\n",
      "\tModel: ibm-granite/granite-3.2-8b-instruct\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 4096}\n",
      "\tstream: True\n"
     ]
    }
   ],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "import sys\n",
    "sys.path.append('..')  \n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint\n",
    "\n",
    "remote = os.getenv(\"REMOTE\", \"True\")\n",
    "\n",
    "if remote == \"False\":\n",
    "    local_port = os.getenv(\"LOCAL_SERVER_PORT\", 8321)\n",
    "    base_url = f\"http://localhost:{local_port}\"\n",
    "else: # any value non equal to 'False' will be considered as 'True'\n",
    "    base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "    \n",
    "print(f\"Connected to Llama Stack server @ {base_url}\")\n",
    "\n",
    "# model_id will later be used to pass the name of the desired inference model to Llama Stack Agents/Inference APIs\n",
    "model_id = os.getenv(\"INFERENCE_MODEL_ID\")\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841eaadf-f5ac-4d7c-bb9d-f039ccd8d9a3",
   "metadata": {},
   "source": [
    "Finally, we will initialize the document collection to be used for RAG ingestion and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c725c2da-05e5-474f-9a44-cf5615557665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87510929-fe4b-428c-8f9e-14d47a03daa2",
   "metadata": {},
   "source": [
    "## 2. Indexing the Documents\n",
    "- Initialize a new document collection in the target vector DB. All parameters related to the vector DB, such as the embedding model and dimension, must be specified here.\n",
    "- Provide a list of document URLs to the RAG tool. Llama Stack will handle fetching, conversion and chunking of the documents' content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d81ffb2-2089-4cb8-adae-f32965f206c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=os.getenv(\"VDB_EMBEDDING\"),\n",
    "    embedding_dimension=int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384)),\n",
    "    provider_id=os.getenv(\"VDB_PROVIDER\"),\n",
    ")\n",
    "\n",
    "# ingest the documents into the newly created document collection\n",
    "urls = [\n",
    "    (\"https://www.openshift.guide/openshift-guide-screen.pdf\", \"application/pdf\"),\n",
    "]\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=url,\n",
    "        mime_type=url_type,\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, (url, url_type) in enumerate(urls)\n",
    "]\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
   "metadata": {},
   "source": [
    "## 3. Executing Queries via the Built-in RAG Tool\n",
    "- Directly invoke the RAG tool to query the vector DB we ingested into at the previous stage.\n",
    "- Construct an extended prompt using the retrieved chunks.\n",
    "- Query the model with the extended prompt.\n",
    "- Output the reply received from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d39ab00-2a65-4b72-b5ed-4dd61f1204a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "User> How to install OpenShift?\u001b[0m\n",
      "\u001b[33mTo\u001b[0m\u001b[33m install\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m follow\u001b[0m\u001b[33m these\u001b[0m\u001b[33m steps\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mDownload\u001b[0m\u001b[33m and\u001b[0m\u001b[33m Install\u001b[0m\u001b[33m the\u001b[0m\u001b[33m oc\u001b[0m\u001b[33m CLI\u001b[0m\u001b[33m Tool\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m You\u001b[0m\u001b[33m need\u001b[0m\u001b[33m to\u001b[0m\u001b[33m download\u001b[0m\u001b[33m and\u001b[0m\u001b[33m install\u001b[0m\u001b[33m the\u001b[0m\u001b[33m `\u001b[0m\u001b[33moc\u001b[0m\u001b[33m`\u001b[0m\u001b[33m command\u001b[0m\u001b[33m-line\u001b[0m\u001b[33m tool\u001b[0m\u001b[33m from\u001b[0m\u001b[33m the\u001b[0m\u001b[33m official\u001b[0m\u001b[33m Red\u001b[0m\u001b[33m Hat\u001b[0m\u001b[33m website\u001b[0m\u001b[33m.\n",
      "\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mCreate\u001b[0m\u001b[33m a\u001b[0m\u001b[33m Cluster\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Create\u001b[0m\u001b[33m an\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m cluster\u001b[0m\u001b[33m using\u001b[0m\u001b[33m one\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m following\u001b[0m\u001b[33m methods\u001b[0m\u001b[33m:\n",
      "\u001b[33m*\u001b[0m\u001b[33m **\u001b[0m\u001b[33mMin\u001b[0m\u001b[33mish\u001b[0m\u001b[33mift\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Use\u001b[0m\u001b[33m Min\u001b[0m\u001b[33mish\u001b[0m\u001b[33mift\u001b[0m\u001b[33m,\u001b[0m\u001b[33m a\u001b[0m\u001b[33m tool\u001b[0m\u001b[33m that\u001b[0m\u001b[33m allows\u001b[0m\u001b[33m you\u001b[0m\u001b[33m to\u001b[0m\u001b[33m run\u001b[0m\u001b[33m an\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m cluster\u001b[0m\u001b[33m on\u001b[0m\u001b[33m your\u001b[0m\u001b[33m local\u001b[0m\u001b[33m machine\u001b[0m\u001b[33m.\n",
      "\u001b[33m*\u001b[0m\u001b[33m **\u001b[0m\u001b[33mOK\u001b[0m\u001b[33mD\u001b[0m\u001b[33m (\u001b[0m\u001b[33mOpen\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m Origin\u001b[0m\u001b[33m)**\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Install\u001b[0m\u001b[33m OK\u001b[0m\u001b[33mD\u001b[0m\u001b[33m,\u001b[0m\u001b[33m which\u001b[0m\u001b[33m is\u001b[0m\u001b[33m an\u001b[0m\u001b[33m open\u001b[0m\u001b[33m-source\u001b[0m\u001b[33m version\u001b[0m\u001b[33m of\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m.\n",
      "\u001b[33m\t\u001b[0m\u001b[33m*\u001b[0m\u001b[33m **\u001b[0m\u001b[33mRed\u001b[0m\u001b[33m Hat\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m Container\u001b[0m\u001b[33m Platform\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Purchase\u001b[0m\u001b[33m and\u001b[0m\u001b[33m install\u001b[0m\u001b[33m the\u001b[0m\u001b[33m full\u001b[0m\u001b[33m Red\u001b[0m\u001b[33m Hat\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m platform\u001b[0m\u001b[33m.\n",
      "\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mConfigure\u001b[0m\u001b[33m Your\u001b[0m\u001b[33m Cluster\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Configure\u001b[0m\u001b[33m your\u001b[0m\u001b[33m cluster\u001b[0m\u001b[33m by\u001b[0m\u001b[33m setting\u001b[0m\u001b[33m up\u001b[0m\u001b[33m a\u001b[0m\u001b[33m project\u001b[0m\u001b[33m,\u001b[0m\u001b[33m creating\u001b[0m\u001b[33m a\u001b[0m\u001b[33m namespace\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m configuring\u001b[0m\u001b[33m networking\u001b[0m\u001b[33m.\n",
      "\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mDeploy\u001b[0m\u001b[33m Applications\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Deploy\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m to\u001b[0m\u001b[33m your\u001b[0m\u001b[33m cluster\u001b[0m\u001b[33m using\u001b[0m\u001b[33m one\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m following\u001b[0m\u001b[33m methods\u001b[0m\u001b[33m:\n",
      "\u001b[33m*\u001b[0m\u001b[33m **\u001b[0m\u001b[33mUsing\u001b[0m\u001b[33m YAML\u001b[0m\u001b[33m Manifest\u001b[0m\u001b[33ms\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Create\u001b[0m\u001b[33m a\u001b[0m\u001b[33m YAML\u001b[0m\u001b[33m file\u001b[0m\u001b[33m that\u001b[0m\u001b[33m defines\u001b[0m\u001b[33m your\u001b[0m\u001b[33m application\u001b[0m\u001b[33m's\u001b[0m\u001b[33m configuration\u001b[0m\u001b[33m and\u001b[0m\u001b[33m deploy\u001b[0m\u001b[33m it\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m cluster\u001b[0m\u001b[33m.\n",
      "\u001b[33m\t\u001b[0m\u001b[33m*\u001b[0m\u001b[33m **\u001b[0m\u001b[33mUsing\u001b[0m\u001b[33m the\u001b[0m\u001b[33m Web\u001b[0m\u001b[33m Console\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Use\u001b[0m\u001b[33m the\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m web\u001b[0m\u001b[33m console\u001b[0m\u001b[33m to\u001b[0m\u001b[33m deploy\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m from\u001b[0m\u001b[33m a\u001b[0m\u001b[33m container\u001b[0m\u001b[33m image\u001b[0m\u001b[33m or\u001b[0m\u001b[33m a\u001b[0m\u001b[33m Git\u001b[0m\u001b[33m repository\u001b[0m\u001b[33m.\n",
      "\u001b[33m\t\u001b[0m\u001b[33m*\u001b[0m\u001b[33m **\u001b[0m\u001b[33mUsing\u001b[0m\u001b[33m o\u001b[0m\u001b[33mdo\u001b[0m\u001b[33m Tool\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Use\u001b[0m\u001b[33m the\u001b[0m\u001b[33m `\u001b[0m\u001b[33modo\u001b[0m\u001b[33m`\u001b[0m\u001b[33m tool\u001b[0m\u001b[33m,\u001b[0m\u001b[33m which\u001b[0m\u001b[33m is\u001b[0m\u001b[33m a\u001b[0m\u001b[33m command\u001b[0m\u001b[33m-line\u001b[0m\u001b[33m interface\u001b[0m\u001b[33m for\u001b[0m\u001b[33m creating\u001b[0m\u001b[33m and\u001b[0m\u001b[33m deploying\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m on\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[33mNote\u001b[0m\u001b[33m:\u001b[0m\u001b[33m The\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m steps\u001b[0m\u001b[33m may\u001b[0m\u001b[33m vary\u001b[0m\u001b[33m depending\u001b[0m\u001b[33m on\u001b[0m\u001b[33m your\u001b[0m\u001b[33m environment\u001b[0m\u001b[33m and\u001b[0m\u001b[33m requirements\u001b[0m\u001b[33m.\u001b[0m\u001b[33m It\u001b[0m\u001b[33m's\u001b[0m\u001b[33m recommended\u001b[0m\u001b[33m to\u001b[0m\u001b[33m refer\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m official\u001b[0m\u001b[33m Red\u001b[0m\u001b[33m Hat\u001b[0m\u001b[33m documentation\u001b[0m\u001b[33m for\u001b[0m\u001b[33m detailed\u001b[0m\u001b[33m instructions\u001b[0m\u001b[33m.\u001b[0m\u001b[33m\u001b[0m"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"How to install OpenShift?\",\n",
    "]\n",
    "\n",
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # RAG retrieval call\n",
    "    rag_response = client.tool_runtime.rag_tool.query(content=prompt, vector_db_ids=[vector_db_id])\n",
    "\n",
    "    # the list of messages to be sent to the model must start with the system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "    # construct the actual prompt to be executed, incorporating the original query and the retrieved content\n",
    "    prompt_context = rag_response.content\n",
    "    extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{prompt_context}\\n\\nQUERY:\\n{prompt}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "    # use Llama Stack inference API to directly communicate with the desired model\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=messages,\n",
    "        model_id=model_id,\n",
    "        sampling_params=sampling_params,\n",
    "        stream=stream,\n",
    "    )\n",
    "    \n",
    "    # print the response\n",
    "    cprint(\"inference> \", color=\"yellow\", end='')\n",
    "    for chunk in response:\n",
    "        response_delta = chunk.event.delta\n",
    "        if isinstance(response_delta, TextDelta):\n",
    "            cprint(response_delta.text, color=\"yellow\", end='')\n",
    "        elif isinstance(response_delta, ToolCallDelta):\n",
    "            cprint(response_delta.tool_call, color=\"yellow\", end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6937a3-3efa-4b66-aaf0-85d96b6d43db",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "This tutorial demonstrates how to set up and use the built-in RAG tool for ingesting user-provided documents in a vector DB and later utilizing them during inference via direct retrieval. Please check out our [complementary tutorial]([Level3_agentic_RAG.ipynb](demos/rag_agentic/notebooks/Level3_agentic_RAG.ipynb) for an agentic RAG example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
